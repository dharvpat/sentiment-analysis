{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, Flatten, Dense, MaxPooling1D, Input\n",
    "import numpy as np\n",
    "import json\n",
    "import spacy\n",
    "import string\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "limiter = 100000\n",
    "\n",
    "def filter_sentence(sentence):\n",
    "    try:\n",
    "        doc = nlp(sentence)\n",
    "        # Include unigrams, bigrams, and trigrams\n",
    "        filtered_tokens = [token.text for token in doc if not token.is_stop and token.text not in string.punctuation and token.text.strip()]\n",
    "        filtered_token = []\n",
    "        filtered_token.extend([' '.join(gram) for gram in zip(filtered_tokens, filtered_tokens[1:])] + \n",
    "                              [' '.join(gram) for gram in zip(filtered_tokens, filtered_tokens[1:], filtered_tokens[2:])])\n",
    "        return filtered_token\n",
    "    except Exception as e:\n",
    "        # Detailed logging\n",
    "        print(f\"Error processing sentence: {sentence}. Error: {str(e)}\")\n",
    "        raise  # Re-raise the exception for a more detailed traceback\n",
    "\n",
    "def calculate_accuracy(Y_true, Y_preds):\n",
    "    correct_predictions = 0\n",
    "    total_samples = len(Y_true)\n",
    "\n",
    "    for i in range(total_samples):\n",
    "        true_class = np.argmax(Y_true[i])\n",
    "        pred_class = np.argmax(Y_preds[i])\n",
    "\n",
    "        if true_class == pred_class:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return accuracy\n",
    "\n",
    "def train_test_split(X,Y):\n",
    "    # Initialize empty lists for each category\n",
    "    category_1_indices = []\n",
    "    category_2_indices = []\n",
    "    category_3_indices = []\n",
    "   # Iterate through the indices and categorize them\n",
    "    Y = np.array(Y)\n",
    "    max_split_index = int(len(Y)*0.75)\n",
    "    for i in range(max_split_index):\n",
    "        val = Y[i]\n",
    "        if val[0] ==  1:\n",
    "            category_1_indices.append(i)\n",
    "        elif val[1] == 1:\n",
    "            category_2_indices.append(i)\n",
    "        elif val[2] == 1:\n",
    "            category_3_indices.append(i)\n",
    "    \n",
    "    neutral_len = len(category_2_indices)\n",
    "    positive_len = len(category_3_indices)\n",
    "    negative_len = len(category_1_indices)\n",
    "\n",
    "    length_of_each = min(neutral_len, negative_len, positive_len)\n",
    "    \n",
    "    negative_indices = category_1_indices[:length_of_each]\n",
    "    neutral_indices = category_2_indices[:length_of_each]\n",
    "    positive_indices = category_3_indices[:length_of_each]\n",
    "    X= np.array(X)\n",
    "    mask = np.ones(len(X), dtype=bool)\n",
    "    mask[negative_indices] = False\n",
    "    mask[positive_indices] = False\n",
    "    mask[neutral_indices] = False\n",
    "    X_train = np.concatenate([X[negative_indices], X[neutral_indices], X[positive_indices]], axis = 0)\n",
    "    Y_train = np.concatenate([Y[negative_indices], Y[neutral_indices], Y[positive_indices]], axis = 0)\n",
    "    X_test = np.concatenate([X[mask], X[max_split_index:]], axis = 0)\n",
    "    Y_test = np.concatenate([Y[mask], Y[max_split_index:]], axis = 0)\n",
    "    return X_train.astype(np.float16), Y_train.astype(np.float16), X_test.astype(np.float16), Y_test.astype(np.float16)\n",
    "\n",
    "\n",
    "\n",
    "# Define the CNN_Text model in TensorFlow\n",
    "class CNN_Text(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, vector_size, n_filters, filter_sizes, output_dim, dropout, input_len):\n",
    "        super(CNN_Text, self).__init__()\n",
    "\n",
    "        # Create word embeddings from the input words\n",
    "        self.embedding = layers.Embedding(input_dim=vocab_size, output_dim=vector_size, input_length=input_len, mask_zero=True)\n",
    "\n",
    "        # Specify convolutions with filters of different sizes (fs)\n",
    "        self.convs = [layers.Conv1D(filters=n_filters, kernel_size=(fs), activation='sigmoid') for fs in filter_sizes]\n",
    "\n",
    "        # Add a fully connected layer for final predictions\n",
    "        self.linear = layers.Dense(output_dim, activation = 'softmax')\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Get word embeddings and format them for convolutions\n",
    "        inputs = inputs[0]\n",
    "        embedded = self.embedding(inputs)\n",
    "\n",
    "        print('Embedding shape: {}'.format(embedded.shape))\n",
    "\n",
    "        # Perform convolutions and apply activation functions\n",
    "        conved = [conv(embedded) for conv in self.convs]\n",
    "\n",
    "        # Pooling layer to reduce dimensionality\n",
    "        pooled = [layers.Flatten()(tf.reduce_max(conv, axis=[1, 2])) for conv in conved]\n",
    "\n",
    "        cat = self.dropout(tf.concat(pooled, axis=1))\n",
    "\n",
    "        cat = self.linear(pooled)\n",
    "\n",
    "        return cat\n",
    "\n",
    "def get_vocab_size(corpus):\n",
    "    flattened = [item for sublist in corpus for item in sublist]\n",
    "    unique_nums = set(flattened)\n",
    "    vocab_size = len(unique_nums)\n",
    "    return vocab_size\n",
    "\n",
    "dicts = []\n",
    "with open('AMAZON_FASHION.json', 'r') as file:\n",
    "    for line in file:\n",
    "        dicts.append(json.loads(line))\n",
    "\n",
    "ratings = []\n",
    "Review_content = []\n",
    "\n",
    "for dict in dicts:\n",
    "    i = 0\n",
    "    data = dict\n",
    "    if 'reviewText' in data and 'overall' in data:\n",
    "        ratings.append(data['overall'])\n",
    "        Review_content.append(data['reviewText'])\n",
    "\n",
    "Review_content = Review_content[:limiter]\n",
    "ratings = ratings[:limiter]\n",
    "\n",
    "filtered_sentences = [filter_sentence(sentence) for sentence in Review_content]\n",
    "\n",
    "vocab_size = get_vocab_size(filtered_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "vector_size = 10\n",
    "word2vec_model = Word2Vec(sentences=filtered_sentences, vector_size=vector_size, window=10, min_count=1, workers=10)\n",
    "\n",
    "X_sequences = []\n",
    "for sentence in filtered_sentences:\n",
    "    sentence_vectors = [word2vec_model.wv[word] for word in sentence]\n",
    "    X_sequences.append(sentence_vectors)\n",
    "max_len = 0\n",
    "for i in X_sequences:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "max_words = max_len  \n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_words, padding='post', dtype=object)\n",
    "del X_sequences\n",
    "ratings = np.array(ratings)\n",
    "new_np_array = np.zeros((len(ratings), 3))\n",
    "for i in range(len(ratings)):\n",
    "    input_arr = np.zeros(3)\n",
    "    val = ratings[i]\n",
    "    if val == 1:\n",
    "        input_arr[0] = 1\n",
    "    elif val == 2:\n",
    "        input_arr[0] = 1\n",
    "    elif val == 3:\n",
    "        input_arr[1] = 1\n",
    "    elif val == 4:\n",
    "        input_arr[2] = 1\n",
    "    elif val == 5:\n",
    "        input_arr[2] = 1\n",
    "    else:\n",
    "        print('error')\n",
    "    new_np_array[i,:] = input_arr\n",
    "Y = new_np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the CNN_Text model\n",
    "X_padded = np.transpose(X_padded, axes=[0,2,1])\n",
    "x_train, y_train, x_test, y_test = train_test_split(X_padded, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_12 (Conv1D)          (None, 10, 200)           433400    \n",
      "                                                                 \n",
      " max_pooling1d_8 (MaxPooling  (None, 5, 200)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_13 (Conv1D)          (None, 5, 75)             45075     \n",
      "                                                                 \n",
      " max_pooling1d_9 (MaxPooling  (None, 2, 75)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_14 (Conv1D)          (None, 2, 50)             11300     \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 490,078\n",
      "Trainable params: 490,078\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1286/1286 [==============================] - 333s 256ms/step - loss: 1.1047 - accuracy: 0.3352 - val_loss: 1.0346 - val_accuracy: 0.0940\n",
      "Epoch 2/100\n",
      "1286/1286 [==============================] - 320s 249ms/step - loss: 1.1038 - accuracy: 0.3386 - val_loss: 1.0480 - val_accuracy: 0.8520\n",
      "Epoch 3/100\n",
      "1286/1286 [==============================] - 320s 249ms/step - loss: 1.1037 - accuracy: 0.3275 - val_loss: 1.0738 - val_accuracy: 0.0940\n",
      "Epoch 4/100\n",
      "1286/1286 [==============================] - 321s 249ms/step - loss: 1.1023 - accuracy: 0.3339 - val_loss: 1.0216 - val_accuracy: 0.8520\n",
      "Epoch 5/100\n",
      "1286/1286 [==============================] - 320s 249ms/step - loss: 1.1023 - accuracy: 0.3397 - val_loss: 1.1464 - val_accuracy: 0.0543\n",
      "Epoch 6/100\n",
      "1286/1286 [==============================] - 322s 251ms/step - loss: 1.0971 - accuracy: 0.3519 - val_loss: 0.9983 - val_accuracy: 0.8482\n",
      "Epoch 7/100\n",
      "1286/1286 [==============================] - 325s 253ms/step - loss: 1.0882 - accuracy: 0.3751 - val_loss: 1.2749 - val_accuracy: 0.0545\n",
      "Epoch 8/100\n",
      "1286/1286 [==============================] - 320s 249ms/step - loss: 1.0733 - accuracy: 0.3958 - val_loss: 0.9524 - val_accuracy: 0.8421\n",
      "Epoch 9/100\n",
      "1286/1286 [==============================] - 316s 246ms/step - loss: 1.0599 - accuracy: 0.4054 - val_loss: 1.3749 - val_accuracy: 0.0947\n",
      "Epoch 10/100\n",
      "1286/1286 [==============================] - 318s 248ms/step - loss: 1.0444 - accuracy: 0.4222 - val_loss: 1.2417 - val_accuracy: 0.1049\n",
      "Epoch 11/100\n",
      "1286/1286 [==============================] - 319s 248ms/step - loss: 1.0312 - accuracy: 0.4327 - val_loss: 1.2080 - val_accuracy: 0.1039\n",
      "Epoch 12/100\n",
      "1286/1286 [==============================] - 322s 250ms/step - loss: 1.0173 - accuracy: 0.4460 - val_loss: 1.0799 - val_accuracy: 0.8292\n",
      "Epoch 13/100\n",
      "1286/1286 [==============================] - 321s 250ms/step - loss: 1.0003 - accuracy: 0.4587 - val_loss: 1.1905 - val_accuracy: 0.1062\n",
      "Epoch 14/100\n",
      "1286/1286 [==============================] - 318s 248ms/step - loss: 0.9814 - accuracy: 0.4718 - val_loss: 1.0853 - val_accuracy: 0.8310\n",
      "Epoch 15/100\n",
      "1286/1286 [==============================] - 320s 249ms/step - loss: 0.9608 - accuracy: 0.4854 - val_loss: 1.3196 - val_accuracy: 0.0975\n",
      "Epoch 16/100\n",
      "1286/1286 [==============================] - 317s 247ms/step - loss: 0.9408 - accuracy: 0.4984 - val_loss: 1.0826 - val_accuracy: 0.0699\n",
      "Epoch 17/100\n",
      "1286/1286 [==============================] - 318s 247ms/step - loss: 0.9219 - accuracy: 0.5106 - val_loss: 1.0186 - val_accuracy: 0.8353\n",
      "Epoch 18/100\n",
      "1286/1286 [==============================] - 315s 245ms/step - loss: 0.9006 - accuracy: 0.5246 - val_loss: 1.2379 - val_accuracy: 0.1012\n",
      "Epoch 19/100\n",
      "1286/1286 [==============================] - 315s 245ms/step - loss: 0.8813 - accuracy: 0.5368 - val_loss: 1.1828 - val_accuracy: 0.1095\n",
      "Epoch 20/100\n",
      "1286/1286 [==============================] - 318s 248ms/step - loss: 0.8615 - accuracy: 0.5489 - val_loss: 1.3595 - val_accuracy: 0.0992\n",
      "Epoch 21/100\n",
      "1286/1286 [==============================] - 322s 251ms/step - loss: 0.8444 - accuracy: 0.5589 - val_loss: 1.0160 - val_accuracy: 0.8342\n",
      "Epoch 22/100\n",
      "1286/1286 [==============================] - 319s 248ms/step - loss: 0.8302 - accuracy: 0.5696 - val_loss: 1.2403 - val_accuracy: 0.1054\n",
      "Epoch 23/100\n",
      "1286/1286 [==============================] - 321s 250ms/step - loss: 0.8123 - accuracy: 0.5827 - val_loss: 1.1159 - val_accuracy: 0.0704\n",
      "Epoch 24/100\n",
      "1286/1286 [==============================] - 330s 257ms/step - loss: 0.7968 - accuracy: 0.5915 - val_loss: 1.0086 - val_accuracy: 0.8329\n",
      "Epoch 25/100\n",
      "1286/1286 [==============================] - 329s 256ms/step - loss: 0.7829 - accuracy: 0.5946 - val_loss: 1.5061 - val_accuracy: 0.1008\n",
      "Epoch 26/100\n",
      "1286/1286 [==============================] - 328s 255ms/step - loss: 0.7711 - accuracy: 0.6049 - val_loss: 1.1554 - val_accuracy: 0.1147\n",
      "Epoch 27/100\n",
      "1286/1286 [==============================] - 325s 253ms/step - loss: 0.7543 - accuracy: 0.6129 - val_loss: 1.1151 - val_accuracy: 0.1149\n",
      "Epoch 28/100\n",
      "1286/1286 [==============================] - 325s 253ms/step - loss: 0.7421 - accuracy: 0.6217 - val_loss: 1.0448 - val_accuracy: 0.1194\n",
      "Epoch 29/100\n",
      "1286/1286 [==============================] - 325s 253ms/step - loss: 0.7331 - accuracy: 0.6300 - val_loss: 1.1575 - val_accuracy: 0.1097\n",
      "Epoch 30/100\n",
      "1286/1286 [==============================] - 321s 250ms/step - loss: 0.7166 - accuracy: 0.6394 - val_loss: 1.1275 - val_accuracy: 0.1142\n",
      "Epoch 31/100\n",
      "1286/1286 [==============================] - 323s 252ms/step - loss: 0.7049 - accuracy: 0.6424 - val_loss: 1.0805 - val_accuracy: 0.8254\n",
      "Epoch 32/100\n",
      "1286/1286 [==============================] - 326s 253ms/step - loss: 0.6929 - accuracy: 0.6511 - val_loss: 1.3844 - val_accuracy: 0.1034\n",
      "Epoch 33/100\n",
      "1286/1286 [==============================] - 326s 254ms/step - loss: 0.6864 - accuracy: 0.6550 - val_loss: 1.3520 - val_accuracy: 0.1054\n",
      "Epoch 34/100\n",
      "1286/1286 [==============================] - 325s 253ms/step - loss: 0.6741 - accuracy: 0.6607 - val_loss: 1.3676 - val_accuracy: 0.1043\n",
      "Epoch 35/100\n",
      "1286/1286 [==============================] - 326s 254ms/step - loss: 0.6613 - accuracy: 0.6697 - val_loss: 1.0595 - val_accuracy: 0.8258\n",
      "Epoch 36/100\n",
      "1286/1286 [==============================] - 316s 246ms/step - loss: 0.6504 - accuracy: 0.6749 - val_loss: 1.1885 - val_accuracy: 0.1141\n",
      "Epoch 37/100\n",
      "1286/1286 [==============================] - 313s 243ms/step - loss: 0.6424 - accuracy: 0.6769 - val_loss: 0.9669 - val_accuracy: 0.8357\n",
      "Epoch 38/100\n",
      "1286/1286 [==============================] - 312s 243ms/step - loss: 0.6337 - accuracy: 0.6804 - val_loss: 1.5823 - val_accuracy: 0.0991\n",
      "Epoch 39/100\n",
      "1286/1286 [==============================] - 317s 246ms/step - loss: 0.6265 - accuracy: 0.6857 - val_loss: 0.8869 - val_accuracy: 0.8367\n",
      "Epoch 40/100\n",
      "1286/1286 [==============================] - 318s 247ms/step - loss: 0.6189 - accuracy: 0.6913 - val_loss: 1.4052 - val_accuracy: 0.1067\n",
      "Epoch 41/100\n",
      "1286/1286 [==============================] - 318s 247ms/step - loss: 0.6052 - accuracy: 0.7001 - val_loss: 1.0209 - val_accuracy: 0.8328\n",
      "Epoch 42/100\n",
      "1286/1286 [==============================] - 318s 247ms/step - loss: 0.5993 - accuracy: 0.7002 - val_loss: 0.8284 - val_accuracy: 0.8394\n",
      "Epoch 43/100\n",
      "1286/1286 [==============================] - 319s 248ms/step - loss: 0.5945 - accuracy: 0.7046 - val_loss: 1.3009 - val_accuracy: 0.0655\n",
      "Epoch 44/100\n",
      "1286/1286 [==============================] - 315s 245ms/step - loss: 0.5839 - accuracy: 0.7101 - val_loss: 1.1619 - val_accuracy: 0.1126\n",
      "Epoch 45/100\n",
      "1286/1286 [==============================] - 317s 246ms/step - loss: 0.5752 - accuracy: 0.7163 - val_loss: 1.1769 - val_accuracy: 0.1125\n",
      "Epoch 46/100\n",
      "1286/1286 [==============================] - 318s 247ms/step - loss: 0.5697 - accuracy: 0.7192 - val_loss: 1.0590 - val_accuracy: 0.8339\n",
      "Epoch 47/100\n",
      "1286/1286 [==============================] - 316s 246ms/step - loss: 0.5641 - accuracy: 0.7242 - val_loss: 1.0093 - val_accuracy: 0.1220\n",
      "Epoch 48/100\n",
      "1286/1286 [==============================] - 317s 247ms/step - loss: 0.5594 - accuracy: 0.7233 - val_loss: 1.2559 - val_accuracy: 0.0679\n",
      "Epoch 49/100\n",
      "1286/1286 [==============================] - 313s 244ms/step - loss: 0.5512 - accuracy: 0.7272 - val_loss: 1.2576 - val_accuracy: 0.1115\n",
      "Epoch 50/100\n",
      "1286/1286 [==============================] - 314s 245ms/step - loss: 0.5468 - accuracy: 0.7300 - val_loss: 1.5049 - val_accuracy: 0.1040\n",
      "Epoch 51/100\n",
      "1286/1286 [==============================] - 313s 244ms/step - loss: 0.5398 - accuracy: 0.7333 - val_loss: 1.1770 - val_accuracy: 0.1145\n",
      "Epoch 52/100\n",
      "1286/1286 [==============================] - 314s 244ms/step - loss: 0.5295 - accuracy: 0.7396 - val_loss: 0.9544 - val_accuracy: 0.8343\n",
      "Epoch 53/100\n",
      "1286/1286 [==============================] - 317s 247ms/step - loss: 0.5221 - accuracy: 0.7456 - val_loss: 1.3094 - val_accuracy: 0.1080\n",
      "Epoch 54/100\n",
      "1286/1286 [==============================] - 313s 243ms/step - loss: 0.5239 - accuracy: 0.7447 - val_loss: 1.0467 - val_accuracy: 0.8313\n",
      "Epoch 55/100\n",
      "1286/1286 [==============================] - 313s 243ms/step - loss: 0.5170 - accuracy: 0.7478 - val_loss: 1.5443 - val_accuracy: 0.1083\n",
      "Epoch 56/100\n",
      "1286/1286 [==============================] - 316s 246ms/step - loss: 0.5163 - accuracy: 0.7488 - val_loss: 1.5079 - val_accuracy: 0.0639\n",
      "Epoch 57/100\n",
      "1286/1286 [==============================] - 320s 249ms/step - loss: 0.5074 - accuracy: 0.7510 - val_loss: 1.3239 - val_accuracy: 0.1141\n",
      "Epoch 58/100\n",
      "1286/1286 [==============================] - 320s 249ms/step - loss: 0.5028 - accuracy: 0.7566 - val_loss: 0.9918 - val_accuracy: 0.8333\n",
      "Epoch 59/100\n",
      "1286/1286 [==============================] - 318s 248ms/step - loss: 0.4960 - accuracy: 0.7581 - val_loss: 1.1266 - val_accuracy: 0.1183\n",
      "Epoch 60/100\n",
      "1286/1286 [==============================] - 313s 244ms/step - loss: 0.4945 - accuracy: 0.7596 - val_loss: 1.1443 - val_accuracy: 0.1170\n",
      "Epoch 61/100\n",
      "1286/1286 [==============================] - 313s 244ms/step - loss: 0.4886 - accuracy: 0.7611 - val_loss: 1.1137 - val_accuracy: 0.1170\n",
      "Epoch 62/100\n",
      "1286/1286 [==============================] - 311s 242ms/step - loss: 0.4812 - accuracy: 0.7656 - val_loss: 1.1168 - val_accuracy: 0.0730\n",
      "Epoch 63/100\n",
      "1286/1286 [==============================] - 312s 242ms/step - loss: 0.4779 - accuracy: 0.7666 - val_loss: 1.1068 - val_accuracy: 0.8284\n",
      "Epoch 64/100\n",
      "1286/1286 [==============================] - 311s 242ms/step - loss: 0.4739 - accuracy: 0.7680 - val_loss: 1.5560 - val_accuracy: 0.1071\n",
      "Epoch 65/100\n",
      "1286/1286 [==============================] - 312s 243ms/step - loss: 0.4676 - accuracy: 0.7733 - val_loss: 1.3877 - val_accuracy: 0.1063\n",
      "Epoch 66/100\n",
      "1286/1286 [==============================] - 312s 243ms/step - loss: 0.4658 - accuracy: 0.7744 - val_loss: 1.2892 - val_accuracy: 0.1128\n",
      "Epoch 67/100\n",
      "1286/1286 [==============================] - 313s 243ms/step - loss: 0.4583 - accuracy: 0.7758 - val_loss: 1.0346 - val_accuracy: 0.8317\n",
      "Epoch 68/100\n",
      "1286/1286 [==============================] - 315s 245ms/step - loss: 0.4633 - accuracy: 0.7769 - val_loss: 1.2755 - val_accuracy: 0.1139\n",
      "Epoch 69/100\n",
      "1286/1286 [==============================] - 314s 244ms/step - loss: 0.4521 - accuracy: 0.7819 - val_loss: 1.1981 - val_accuracy: 0.1161\n",
      "Epoch 70/100\n",
      "1286/1286 [==============================] - 317s 247ms/step - loss: 0.4493 - accuracy: 0.7871 - val_loss: 1.3167 - val_accuracy: 0.1147\n",
      "Epoch 71/100\n",
      "1286/1286 [==============================] - 321s 250ms/step - loss: 0.4475 - accuracy: 0.7840 - val_loss: 1.0846 - val_accuracy: 0.8301\n",
      "Epoch 72/100\n",
      "1286/1286 [==============================] - 321s 250ms/step - loss: 0.4453 - accuracy: 0.7858 - val_loss: 1.3199 - val_accuracy: 0.0689\n",
      "Epoch 73/100\n",
      "1286/1286 [==============================] - 320s 249ms/step - loss: 0.4422 - accuracy: 0.7857 - val_loss: 1.1450 - val_accuracy: 0.8274\n",
      "Epoch 74/100\n",
      "1286/1286 [==============================] - 318s 247ms/step - loss: 0.4321 - accuracy: 0.7924 - val_loss: 1.4723 - val_accuracy: 0.1076\n",
      "Epoch 75/100\n",
      "1286/1286 [==============================] - 318s 247ms/step - loss: 0.4387 - accuracy: 0.7904 - val_loss: 1.1790 - val_accuracy: 0.1154\n",
      "Epoch 76/100\n",
      "1286/1286 [==============================] - 320s 249ms/step - loss: 0.4332 - accuracy: 0.7908 - val_loss: 1.0586 - val_accuracy: 0.8315\n",
      "Epoch 77/100\n",
      "1286/1286 [==============================] - 317s 246ms/step - loss: 0.4257 - accuracy: 0.7971 - val_loss: 1.2335 - val_accuracy: 0.0721\n",
      "Epoch 78/100\n",
      "1286/1286 [==============================] - 317s 247ms/step - loss: 0.4261 - accuracy: 0.7956 - val_loss: 1.1614 - val_accuracy: 0.1169\n",
      "Epoch 79/100\n",
      "1286/1286 [==============================] - 320s 249ms/step - loss: 0.4178 - accuracy: 0.7976 - val_loss: 1.3077 - val_accuracy: 0.1115\n",
      "Epoch 80/100\n",
      "1286/1286 [==============================] - 326s 253ms/step - loss: 0.4222 - accuracy: 0.7995 - val_loss: 1.2728 - val_accuracy: 0.1123\n",
      "Epoch 81/100\n",
      "1286/1286 [==============================] - 323s 251ms/step - loss: 0.4154 - accuracy: 0.8021 - val_loss: 1.1284 - val_accuracy: 0.1171\n",
      "Epoch 82/100\n",
      "1286/1286 [==============================] - 316s 246ms/step - loss: 0.4201 - accuracy: 0.7992 - val_loss: 1.2919 - val_accuracy: 0.1134\n",
      "Epoch 83/100\n",
      "1286/1286 [==============================] - 313s 244ms/step - loss: 0.4140 - accuracy: 0.8021 - val_loss: 1.2058 - val_accuracy: 0.0739\n",
      "Epoch 84/100\n",
      "1286/1286 [==============================] - 314s 244ms/step - loss: 0.4066 - accuracy: 0.8063 - val_loss: 1.1010 - val_accuracy: 0.8310\n",
      "Epoch 85/100\n",
      "1286/1286 [==============================] - 314s 245ms/step - loss: 0.4065 - accuracy: 0.8063 - val_loss: 1.1319 - val_accuracy: 0.8269\n",
      "Epoch 86/100\n",
      "1286/1286 [==============================] - 314s 244ms/step - loss: 0.4045 - accuracy: 0.8093 - val_loss: 1.5844 - val_accuracy: 0.1046\n",
      "Epoch 87/100\n",
      "1286/1286 [==============================] - 313s 244ms/step - loss: 0.4032 - accuracy: 0.8092 - val_loss: 1.2595 - val_accuracy: 0.1164\n",
      "Epoch 88/100\n",
      "1286/1286 [==============================] - 313s 244ms/step - loss: 0.4011 - accuracy: 0.8093 - val_loss: 0.9801 - val_accuracy: 0.8329\n",
      "Epoch 89/100\n",
      "1286/1286 [==============================] - 315s 245ms/step - loss: 0.3976 - accuracy: 0.8102 - val_loss: 1.3028 - val_accuracy: 0.1152\n",
      "Epoch 90/100\n",
      "1286/1286 [==============================] - 314s 244ms/step - loss: 0.3915 - accuracy: 0.8142 - val_loss: 1.1450 - val_accuracy: 0.1162\n",
      "Epoch 91/100\n",
      "1286/1286 [==============================] - 314s 244ms/step - loss: 0.3911 - accuracy: 0.8152 - val_loss: 1.0155 - val_accuracy: 0.8305\n",
      "Epoch 92/100\n",
      "1286/1286 [==============================] - 314s 244ms/step - loss: 0.3847 - accuracy: 0.8146 - val_loss: 1.0384 - val_accuracy: 0.8326\n",
      "Epoch 93/100\n",
      "1286/1286 [==============================] - 318s 247ms/step - loss: 0.3908 - accuracy: 0.8133 - val_loss: 1.1161 - val_accuracy: 0.8313\n",
      "Epoch 94/100\n",
      "1286/1286 [==============================] - 314s 244ms/step - loss: 0.3870 - accuracy: 0.8139 - val_loss: 1.1285 - val_accuracy: 0.8284\n",
      "Epoch 95/100\n",
      "1286/1286 [==============================] - 317s 246ms/step - loss: 0.3912 - accuracy: 0.8127 - val_loss: 1.1218 - val_accuracy: 0.1177\n",
      "Epoch 96/100\n",
      "1286/1286 [==============================] - 323s 251ms/step - loss: 0.3845 - accuracy: 0.8198 - val_loss: 1.0780 - val_accuracy: 0.8272\n",
      "Epoch 97/100\n",
      "1286/1286 [==============================] - 326s 254ms/step - loss: 0.3822 - accuracy: 0.8205 - val_loss: 1.2388 - val_accuracy: 0.0716\n",
      "Epoch 98/100\n",
      "1286/1286 [==============================] - 320s 249ms/step - loss: 0.3782 - accuracy: 0.8221 - val_loss: 1.0231 - val_accuracy: 0.8301\n",
      "Epoch 99/100\n",
      "1286/1286 [==============================] - 327s 255ms/step - loss: 0.3790 - accuracy: 0.8219 - val_loss: 0.9941 - val_accuracy: 0.8339\n",
      "Epoch 100/100\n",
      "1286/1286 [==============================] - 328s 255ms/step - loss: 0.3762 - accuracy: 0.8218 - val_loss: 1.1640 - val_accuracy: 0.1165\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(vector_size, max_len)))\n",
    "model.add(Conv1D(filters = 200 , kernel_size = 2, padding = 'same', activation = 'sigmoid'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(filters = 75 , kernel_size = 3, padding = 'same', activation = 'sigmoid'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(filters = 50 , kernel_size = 3, padding = 'same', activation = 'sigmoid'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train,batch_size=16,validation_data=(x_test, y_test), epochs = 100)\n",
    "\n",
    "# Save the model\n",
    "model.save_weights('CNN-model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
